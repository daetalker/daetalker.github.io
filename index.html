
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>
    DAE-Talker
  </title>

  <!-- bootstrap -->
  <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  <!-- icon -->
  <script src="https://kit.fontawesome.com/87dc3e863a.js" crossorigin="anonymous"></script>
  <!-- font -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
  <style>
    body {
      background: rgb(255, 255, 255) no-repeat fixed top left;
      font-family:'Open Sans', sans-serif;
    }
  </style>

</head>

<body>
  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0" style="padding-bottom: 0px;">
      <div class="container-fluid">
        <div class="row">
          <div class="col">
            <!-- paper title -->
            <h2 style="font-size:30px;">
                DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder
            </h2>

            <hr>
            
            <!-- authors -->
            <h6>Chenpeng Du<sup>1</sup>,
            Qi Chen<sup>1</sup>,
            Tianyu He<sup>2</sup>,
            Xu Tan<sup>2</sup>,
            Xie Chen<sup>1</sup>,
            Kai Yu<sup>1</sup>,
            Sheng Zhao<sup>3</sup>,
            Jiang Bian<sup>2</sup>
                 <br>
                 <br>
            <p> <sup>1</sup> X-LANCE Lab, Department of Computer Science and Engineering, Shanghai Jiao Tong University <br>
              <sup>2</sup> Microsoft Research Asia <br>
              <sup>3</sup> Microsoft Cloud+AI <br>
            </p>
            </h6>
            <!-- links -->
           <div class="row justify-content-center">
              <!-- link to paper -->
               <div style="margin: 5px;">
                <p class="mb-5"><a class="btn btn-large btn-dark" href="https://arxiv.org/abs/2303.17550" role="button" target="_blank">
                <i class="fa fa-file"></i>
                Paper
                </a> </p>
              </div>
<!--              &lt;!&ndash; link to video &ndash;&gt;-->
<!--              <div style="margin: 5px;">-->
<!--                <p class="mb-5"><a class="btn btn-large btn-dark" href="https://www.youtube.com/watch?v=xxxxx" role="button" target="_blank">-->
<!--                <i class="fa-brands fa-youtube"></i>-->
<!--                Video-->
<!--                </a> </p>-->
<!--              </div>-->
<!--              &lt;!&ndash; link to code &ndash;&gt;-->
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
          <hr style="margin-top:0px">
        </div>
          <p class="text-justify">
            While recent research has made significant progress in speech-driven talking face generation, the quality of the generated video still lags behind that of real recordings. One reason for this is the use of handcrafted intermediate representations like facial landmarks and 3DMM coefficients, which are designed based on human knowledge and are insufficient to precisely describe facial movements. Additionally, these methods require an external pretrained model for extracting these representations, whose performance sets an upper bound on talking face generation. To address these limitations, we propose a novel method called DAE-Talker that leverages data-driven latent representations obtained from a diffusion autoencoder (DAE). DAE contains an image encoder that encodes an image into a latent vector and a DDIM-based image decoder that reconstructs the image from it. We train our DAE on talking face video frames and then extract their latent representations as the training target for a Conformer-based speech2latent model. During inference, DAE-Talker first predicts the latents from speech and then generates the video frames with the image decoder in DAE from the predicted latents.
This allows DAE-Talker to synthesize full video frames and produce natural head movements that align with the content of speech, rather than relying on a predetermined head pose from a template video. We also introduce pose modelling in speech2latent for pose controllability. Additionally, we propose a novel method for generating continuous video frames with the DDIM-based image decoder trained on individual frames, eliminating the need for modelling the joint distribution of consecutive frames directly. Our experiments show that DAE-Talker outperforms existing popular methods in lip-sync, video fidelity, and pose naturalness. We also conduct ablation studies to analyze the effectiveness of the proposed techniques and demonstrate the pose controllability of DAE-Talker.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>


  <!-- demo -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Demo</h3>
          <hr style="margin-top:0px">
        </div>

          <div  style="text-align:center"> <video width="70%" controls> <source src="resources/demo.mp4" type="video/mp4"> </video> </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  


  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
        <h3>Citation</h3>
        <hr style="margin-top:0px">
        <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>
@inproceedings{daetalker,
  author = {Du, Chenpeng and Chen, Qi and He, Tianyu and Tan, Xu and Chen, Xie and Yu, Kai and Zhao, Sheng and Bian, Jiang},
  title = {DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder},
  year = {2023},
  booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
  pages = {4281â€“4289},
  numpages = {9},
}
</code></pre>
      </div>
    </div>
  </div>


</body>
</html>
